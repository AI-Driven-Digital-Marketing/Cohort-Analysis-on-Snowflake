{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84340ea3",
   "metadata": {},
   "source": [
    "# Cohort Analysis\n",
    "This notebook includes EDA, cohort analysis calculation.    \n",
    "Snowpark session Documentation : https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/session.html    \n",
    "Python Documentation: https://docs.snowflake.com/en/developer-guide/snowpark/python/index    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885bbe2",
   "metadata": {},
   "source": [
    "**examples**\n",
    "Github example:https://github.com/Snowflake-Labs/snowpark-python-demos/tree/main/Advertising-Spend-ROI-Prediction    \n",
    "Jupyter notebook example: https://github.com/Snowflake-Labs/snowpark-python-demos/blob/main/Advertising-Spend-ROI-Prediction/Snowpark_For_Python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f9cc5",
   "metadata": {},
   "source": [
    "#### Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d7fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit,count\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f16a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : ESTPEGION\n",
      "Role                        : ACCOUNTADMIN\n",
      "Database                    : KPMG\n",
      "Schema                      : PUBLIC\n",
      "Warehouse                   : COMPUTE_WH\n",
      "Snowflake version           : 7.6.2\n",
      "Snowpark for Python version : 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(snowflake_environment[0][1]))\n",
    "print('Database                    : {}'.format(snowflake_environment[0][2]))\n",
    "print('Schema                      : {}'.format(snowflake_environment[0][3]))\n",
    "print('Warehouse                   : {}'.format(snowflake_environment[0][5]))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][4]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c22814",
   "metadata": {},
   "source": [
    "#### List all tables in the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a2200c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "|\"TABLE_NAME\"     |\n",
      "-------------------\n",
      "|CUSTOMERADDRESS  |\n",
      "|FOOD             |\n",
      "|TRANSACTIONS     |\n",
      "|DEMOGRAPHIC      |\n",
      "|NEWCUSTOMER      |\n",
      "|TABLES           |\n",
      "|COLUMNS          |\n",
      "|SCHEMATA         |\n",
      "|SEQUENCES        |\n",
      "|VIEWS            |\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('select TABLE_NAME from information_schema.tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba122a4",
   "metadata": {},
   "source": [
    "#### Query table from snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4253cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 14:18:03.064 INFO    snowflake.connector.cursor: query: [SELECT  *  FROM TRANSACTIONS LIMIT 10]\n",
      "2023-02-22 14:18:03.159 INFO    snowflake.connector.cursor: query execution done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"TRANSACTION_ID\"  |\"PRODUCT_ID\"  |\"CUSTOMER_ID\"  |\"TRANSACTION_DATE\"   |\"ONLINE_ORDER\"  |\"ORDER_STATUS\"  |\"BRAND\"         |\"PRODUCT_LINE\"  |\"PRODUCT_CLASS\"  |\"PRODUCT_SIZE\"  |\"LIST_PRICE\"  |\"STANDARD_COST\"  |\"PRODUCT_FIRST_SOLD_DATE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1                 |2             |2950           |2017-02-25 00:00:00  |0.0             |Approved        |Solex           |Standard        |medium           |medium          |71.49         |53.62            |41245.0                    |\n",
      "|2                 |3             |3120           |2017-05-21 00:00:00  |1.0             |Approved        |Trek Bicycles   |Standard        |medium           |large           |2091.47       |388.92           |41701.0                    |\n",
      "|3                 |37            |402            |2017-10-16 00:00:00  |0.0             |Approved        |OHM Cycles      |Standard        |low              |medium          |1793.43       |248.82           |36361.0                    |\n",
      "|4                 |88            |3135           |2017-08-31 00:00:00  |0.0             |Approved        |Norco Bicycles  |Standard        |medium           |medium          |1198.46       |381.1            |36145.0                    |\n",
      "|5                 |78            |787            |2017-10-01 00:00:00  |1.0             |Approved        |Giant Bicycles  |Standard        |medium           |large           |1765.3        |709.48           |42226.0                    |\n",
      "|6                 |25            |2339           |2017-03-08 00:00:00  |1.0             |Approved        |Giant Bicycles  |Road            |medium           |medium          |1538.99       |829.65           |39031.0                    |\n",
      "|7                 |22            |1542           |2017-04-21 00:00:00  |1.0             |Approved        |WeareA2B        |Standard        |medium           |medium          |60.34         |45.26            |34165.0                    |\n",
      "|8                 |15            |2459           |2017-07-15 00:00:00  |0.0             |Approved        |WeareA2B        |Standard        |medium           |medium          |1292.84       |13.44            |39915.0                    |\n",
      "|9                 |67            |1305           |2017-08-10 00:00:00  |0.0             |Approved        |Solex           |Standard        |medium           |large           |1071.23       |380.74           |33455.0                    |\n",
      "|10                |12            |3262           |2017-08-30 00:00:00  |1.0             |Approved        |WeareA2B        |Standard        |medium           |medium          |1231.15       |161.6            |38216.0                    |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend = session.table('TRANSACTIONS')\n",
    "snow_df_spend.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1380910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "|\"COUNT(1)\"  |\n",
      "--------------\n",
      "|20000       |\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate row number\n",
    "snow_df_spend.select(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f10c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import streamlit as st\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit,count\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import json\n",
    "import logging \n",
    "\n",
    "# The code below is for the title and logo for this page.\n",
    "st.set_page_config(page_title=\"Cohort Analysis on the Bikes dataset\", page_icon=\"ðŸš²\")\n",
    "bg_image = '''\n",
    "<style>\n",
    "[data-testid=\"stAppViewContainer\"]{\n",
    "background-image: url(https://lp-cms-production.imgix.net/2022-01/GettyRF_475680439.jpg?auto=format&q=75&w=1920);\n",
    "backgroud-size:cover;\n",
    "}\n",
    "\n",
    "[data-testid=\"stHeader\"]{\n",
    "background-color: rgba(0,0,0,0);\n",
    "}\n",
    "\n",
    "[data-testid=\"stToolbar\"]{\n",
    "right : 2rem;}\n",
    "\n",
    "</style>\n",
    "'''\n",
    "#st.markdown(bg_image, unsafe_allow_html=True)\n",
    "#st.set_page_config(layout=\"wide\")\n",
    "st.sidebar.markdown(\"# Bike Cohort Analysis\")\n",
    "st.image(\n",
    "    \"TrevelBike.jpeg\",\n",
    "    #width  = 160\n",
    "    use_column_width = True,\n",
    ")\n",
    "\n",
    "st.title(\"Cohort Analysis â†’ `Bikes` dataset\")\n",
    "\n",
    "st.write(\"\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "\n",
    "    This demo is inspired by this [Cohort Analysis Tutorial](https://github.com/maladeep/cohort-retention-rate-analysis-in-python).\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "with st.expander(\"About this app\"):\n",
    "\n",
    "    st.write(\"\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "\n",
    "    This dataset comes from the hypothetical KPMG.\n",
    "\n",
    "    Each row in the dataset contains information about an individual bike purchase:\n",
    "\n",
    "    - Who bought it\n",
    "    - How much they paid\n",
    "    - The bike's `brand` and `product line`\n",
    "    - Its `class` and `size`\n",
    "    - What day the purchase happened\n",
    "    - The day the product was first sold\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    st.write(\"\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "    The underlying code groups those purchases into cohorts and calculates the `retention rate` (split by month) so that one can answer the question:\n",
    "\n",
    "    *if I'm making monthly changes to my store to get people to come back and buy more bikes, are those changes working?\"*\n",
    "\n",
    "    These cohorts are then visualized and interpreted through a heatmap [powered by Plotly](https://plotly.com/python/).\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    st.write(\"\")\n",
    "\n",
    "# A function that will parse the date Time based cohort:  1 day of month\n",
    "def get_month(x):\n",
    "    return dt.datetime(x.year, x.month, 1)\n",
    "\n",
    "@st.cache_resource\n",
    "def connect2snowflake():\n",
    "    # set logger\n",
    "    logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    # Create Snowflake Session object\n",
    "    connection_parameters = json.load(open('connection.json'))\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    session.sql_simplifier_enabled = True\n",
    "\n",
    "    snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "    snowpark_version = VERSION\n",
    "    return session\n",
    "session = connect2snowflake()\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "\n",
    "    # Load data\n",
    "    transaction_df = pd.DataFrame(session.table('TRANSACTIONS').collect())\n",
    "    #transaction_df = session.sql('select * from TRANSACTIONS').toPandas()\n",
    "    transaction_df.columns = [x.lower() for x in transaction_df.columns]\n",
    "\n",
    "\n",
    "    # Process data\n",
    "    transaction_df = transaction_df.replace(\" \", np.NaN)\n",
    "    transaction_df = transaction_df.fillna(transaction_df.mean())\n",
    "    transaction_df[\"TransactionMonth\"] = transaction_df[\"transaction_date\"].apply(\n",
    "        get_month\n",
    "    )\n",
    "    transaction_df[\"TransactionYear\"] = transaction_df[\"transaction_date\"].dt.year\n",
    "    transaction_df[\"TransactionMonth\"] = transaction_df[\"transaction_date\"].dt.month\n",
    "    for col in transaction_df.columns:\n",
    "        if transaction_df[col].dtype == \"object\":\n",
    "            transaction_df[col] = transaction_df[col].fillna(\n",
    "                transaction_df[col].value_counts().index[0]\n",
    "            )\n",
    "\n",
    "    # Create transaction_date column based on month and store in TransactionMonth\n",
    "    transaction_df[\"TransactionMonth\"] = transaction_df[\"transaction_date\"].apply(\n",
    "        get_month\n",
    "    )\n",
    "    # Grouping by customer_id and select the InvoiceMonth value\n",
    "    grouping = transaction_df.groupby(\"customer_id\")[\"TransactionMonth\"]\n",
    "    # Assigning a minimum InvoiceMonth value to the dataset\n",
    "    transaction_df[\"CohortMonth\"] = grouping.transform(\"min\")\n",
    "\n",
    "    return transaction_df\n",
    "\n",
    "\n",
    "transaction_df = load_data()\n",
    "\n",
    "with st.expander(\"Show the `Bikes` dataframe\"):\n",
    "    st.write(transaction_df)\n",
    "\n",
    "\n",
    "def get_date_int(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    day = df[column].dt.day\n",
    "    return year, month, day\n",
    "\n",
    "\n",
    "# Getting the integers for date parts from the `InvoiceDay` column\n",
    "transcation_year, transaction_month, _ = get_date_int(\n",
    "    transaction_df, \"TransactionMonth\"\n",
    ")\n",
    "# Getting the integers for date parts from the `CohortDay` column\n",
    "cohort_year, cohort_month, _ = get_date_int(transaction_df, \"CohortMonth\")\n",
    "#  Get the  difference in years\n",
    "years_diff = transcation_year - cohort_year\n",
    "# Calculate difference in months\n",
    "months_diff = transaction_month - cohort_month\n",
    "\n",
    "# Extract the difference in months from all previous values \"+1\" in addeded at the end so that first month is marked as 1 instead of 0 for easier interpretation. \"\"\"\n",
    "transaction_df[\"CohortIndex\"] = years_diff * 12 + months_diff + 1\n",
    "\n",
    "dtypes = transaction_df.dtypes.astype(str)\n",
    "# Show dtypes\n",
    "# dtypes\n",
    "\n",
    "transaction_df_new_slider_01 = transaction_df[[\"brand\", \"product_line\"]]\n",
    "new_slider_01 = [col for col in transaction_df_new_slider_01]\n",
    "\n",
    "transaction_df_new_slider_02 = transaction_df[[\"list_price\", \"standard_cost\"]]\n",
    "new_slider_02 = [col for col in transaction_df_new_slider_02]\n",
    "\n",
    "st.write(\"\")\n",
    "\n",
    "cole, col1, cole, col2, cole = st.columns([0.1, 1, 0.05, 1, 0.1])\n",
    "\n",
    "with col1:\n",
    "\n",
    "    MetricSlider01 = st.selectbox(\"Pick your 1st metric\", new_slider_01)\n",
    "\n",
    "    MetricSlider02 = st.selectbox(\"Pick your 2nd metric\", new_slider_02, index=1)\n",
    "\n",
    "    st.write(\"\")\n",
    "\n",
    "with col2:\n",
    "\n",
    "    if MetricSlider01 == \"brand\":\n",
    "        # col_one_list = transaction_df_new[\"brand\"].tolist()\n",
    "        col_one_list = transaction_df_new_slider_01[\"brand\"].drop_duplicates().tolist()\n",
    "        multiselect = st.multiselect(\n",
    "            \"Select the value(s)\", col_one_list, [\"Solex\", \"Trek Bicycles\"]\n",
    "        )\n",
    "        transaction_df = transaction_df[transaction_df[\"brand\"].isin(multiselect)]\n",
    "\n",
    "    elif MetricSlider01 == \"product_line\":\n",
    "        col_one_list = (\n",
    "            transaction_df_new_slider_01[\"product_line\"].drop_duplicates().tolist()\n",
    "        )\n",
    "        multiselect = st.multiselect(\n",
    "            \"Select the value(s)\", col_one_list, [\"Standard\", \"Road\"]\n",
    "        )\n",
    "        transaction_df = transaction_df[\n",
    "            transaction_df[\"product_line\"].isin(multiselect)\n",
    "        ]\n",
    "\n",
    "    if MetricSlider02 == \"list_price\":\n",
    "        list_price_slider = st.slider(\n",
    "            \"List price (in $)\", step=500, min_value=12, max_value=2091\n",
    "        )\n",
    "        transaction_df = transaction_df[\n",
    "            transaction_df[\"list_price\"] > list_price_slider\n",
    "        ]\n",
    "\n",
    "    elif MetricSlider02 == \"standard_cost\":\n",
    "        standard_cost_slider = st.slider(\n",
    "            \"Standard cost (in $)\", step=500, min_value=7, max_value=1759\n",
    "        )\n",
    "        transaction_df = transaction_df[\n",
    "            transaction_df[\"list_price\"] > standard_cost_slider\n",
    "        ]\n",
    "\n",
    "try:\n",
    "\n",
    "    # Counting daily active user from each chort\n",
    "    grouping = transaction_df.groupby([\"CohortMonth\", \"CohortIndex\"])\n",
    "    # Counting number of unique customer Id's falling in each group of CohortMonth and CohortIndex\n",
    "    cohort_data = grouping[\"customer_id\"].apply(pd.Series.nunique)\n",
    "    cohort_data = cohort_data.reset_index()\n",
    "    # Assigning column names to the dataframe created above\n",
    "    cohort_counts = cohort_data.pivot(\n",
    "        index=\"CohortMonth\", columns=\"CohortIndex\", values=\"customer_id\"\n",
    "    )\n",
    "\n",
    "    cohort_sizes = cohort_counts.iloc[:, 0]\n",
    "    retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
    "    # Coverting the retention rate into percentage and Rounding off.\n",
    "    retention = retention.round(3) * 100\n",
    "    retention.index = retention.index.strftime(\"%Y-%m\")\n",
    "\n",
    "    # Plotting the retention rate\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_heatmap(\n",
    "        # x=retention.columns, y=retention.index, z=retention, colorscale=\"cividis\"\n",
    "        x=retention.columns,\n",
    "        y=retention.index,\n",
    "        z=retention,\n",
    "        # Best\n",
    "        # colorscale=\"Aggrnyl\",\n",
    "        colorscale=\"Bluyl\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title_text=\"Monthly cohorts showing retention rates\", title_x=0.2)\n",
    "    fig.layout.xaxis.title = \"Cohort Group\"\n",
    "    fig.layout.yaxis.title = \"Cohort Period\"\n",
    "    fig[\"layout\"][\"title\"][\"font\"] = dict(size=25)\n",
    "    fig.layout.width = 750\n",
    "    fig.layout.height = 750\n",
    "    fig.layout.xaxis.tickvals = retention.columns\n",
    "    fig.layout.yaxis.tickvals = retention.index\n",
    "    fig.layout.plot_bgcolor = \"#efefef\"  # Set the background color to white\n",
    "    fig.layout.margin.b = 100\n",
    "    fig\n",
    "\n",
    "except IndexError:\n",
    "    st.warning(\"This is throwing an exception, bear with us!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dbf1812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pages/Food_Cohort_Analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pages/Food_Cohort_Analysis.py\n",
    "import streamlit as st\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType, StructField, DateType, Variant\n",
    "from snowflake.snowpark.functions import udf, sum, col,array_construct,month,year,call_udf,lit,count\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import json\n",
    "import logging \n",
    "\n",
    "# The code below is for the title and logo for this page.\n",
    "st.set_page_config(page_title=\"Cohort Analysis on the Food dataset\", page_icon=\"ðŸ”\")\n",
    "\n",
    "st.image(\n",
    "    \"Food.jpg\",\n",
    "    use_column_width = True,\n",
    ")\n",
    "\n",
    "st.title(\"Cohort Analysis â†’ `Food` dataset\")\n",
    "\n",
    "st.write(\"\")\n",
    "\n",
    "\n",
    "\n",
    "with st.expander(\"About this app\"):\n",
    "\n",
    "    st.write(\"\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "\n",
    "    This dataset comes from the hypothetical KPMG.\n",
    "\n",
    "    Each row in the dataset contains information about an individual bike purchase:\n",
    "\n",
    "    - Who bought it\n",
    "    - How much they paid\n",
    "    - The bike's `brand` and `product line`\n",
    "    - Its `class` and `size`\n",
    "    - What day the purchase happened\n",
    "    - The day the product was first sold\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    st.write(\"\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "    The underlying code groups those purchases into cohorts and calculates the `retention rate` (split by month) so that one can answer the question:\n",
    "\n",
    "    *if I'm making monthly changes to my store to get people to come back and buy more bikes, are those changes working?\"*\n",
    "\n",
    "    These cohorts are then visualized and interpreted through a heatmap [powered by Plotly](https://plotly.com/python/).\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    st.write(\"\")\n",
    "\n",
    "# A function that will parse the date Time based cohort:  1 day of month\n",
    "def get_month(x):\n",
    "    return dt.datetime(x.year, x.month, 1)\n",
    "\n",
    "@st.cache_resource\n",
    "def connect2snowflake():\n",
    "    # set logger\n",
    "    logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    # Create Snowflake Session object\n",
    "    connection_parameters = json.load(open('connection.json'))\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    session.sql_simplifier_enabled = True\n",
    "\n",
    "    snowflake_environment = session.sql('select current_user(), current_role(), current_database(), current_schema(), current_version(), current_warehouse()').collect()\n",
    "    snowpark_version = VERSION\n",
    "    return session\n",
    "session = connect2snowflake()\n",
    "\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "\n",
    "    # Load data\n",
    "    food_df = pd.DataFrame(session.table('FOOD').collect())\n",
    "    food_df['ORDERDATE'] = pd.to_datetime(food_df['ORDERDATE'])\n",
    "    #food_df['PICKUPDATE'] = pd.to_datetime(food_df['PICKUPDATE'])\n",
    "    food_df = food_df.replace(\" \",np.NaN)\n",
    "    food_df = food_df.fillna(food_df.mean(numeric_only=True))\n",
    "    #food_df.isna().sum()\n",
    "    food_df['TransactionMonth'] =food_df['ORDERDATE'].apply(lambda x: x.replace(day = 1))\n",
    "    # Grouping by customer_id and select the InvoiceMonth value\n",
    "    grouping = food_df.groupby('USERID')['TransactionMonth'] \n",
    "\n",
    "    # Assigning a minimum InvoiceMonth value to the dataset\n",
    "    food_df['CohortMonth'] = grouping.transform('min')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return food_df\n",
    "\n",
    "\n",
    "food_df = load_data()\n",
    "\n",
    "with st.expander(\"Show the `Food` dataframe\"):\n",
    "    st.write(food_df)\n",
    "\n",
    "@st.cache_data\n",
    "def get_minmaxCharges():\n",
    "    return food_df.TOTALCHARGES.max(), food_df.TOTALCHARGES.min()\n",
    "\n",
    "max_charge, min_charge = get_minmaxCharges()\n",
    "_, col1, _ = st.columns([0.2, 1, 0.2])\n",
    "\n",
    "with col1:\n",
    "    TotalCharges_slider = st.slider(\n",
    "            \"Total Charges (in $)\",  0.0,  max_charge, \n",
    "        )    \n",
    "\n",
    "food_df = food_df[food_df[\"TOTALCHARGES\"] > TotalCharges_slider]\n",
    "def get_date_int(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    day = df[column].dt.day\n",
    "    return year, month, day\n",
    "# Getting the integers for date parts from the `InvoiceDay` column\n",
    "transcation_year, transaction_month, _ = get_date_int(food_df, 'TransactionMonth')\n",
    "\n",
    "# Getting the integers for date parts from the `CohortDay` column\n",
    "cohort_year, cohort_month, _ = get_date_int(food_df, 'CohortMonth')\n",
    "#  Get the  difference in years\n",
    "years_diff = transcation_year - cohort_year\n",
    "\n",
    "# Calculate difference in months\n",
    "months_diff = transaction_month - cohort_month\n",
    "food_df['CohortIndex'] = years_diff * 12 + months_diff  + 1 \n",
    "# Counting daily active user from each chort\n",
    "grouping = food_df.groupby(['CohortMonth', 'CohortIndex'])\n",
    "\n",
    "\n",
    "\n",
    "# Counting number of unique customer Id's falling in each group of CohortMonth and CohortIndex\n",
    "cohort_data = grouping['USERID'].apply(pd.Series.nunique)\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "\n",
    " # Assigning column names to the dataframe created above\n",
    "cohort_counts = cohort_data.pivot(index='CohortMonth',\n",
    "                                 columns ='CohortIndex',\n",
    "                                 values = 'USERID')\n",
    "cohort_sizes = cohort_counts.iloc[:,0]\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
    "retention.round(3)*100\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35eac0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690.98"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a558626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting environment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile environment.yml\n",
    "name: SNPark\n",
    "dependencies:\n",
    "  - snowflake-snowpark-python\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - plotly\n",
    "  - matplotlib\n",
    "  - seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8f02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "def get_month(x):\n",
    "    return dt.datetime(x.year, x.month, 1)\n",
    "def load_data():\n",
    "\n",
    "    # Load data\n",
    "    transaction_df = pd.DataFrame(session.table('TRANSACTIONS').collect())\n",
    "    #transaction_df = session.sql('select * from TRANSACTIONS').toPandas()\n",
    "    transaction_df.columns = [x.lower() for x in transaction_df.columns]\n",
    "\n",
    "\n",
    "    # Process data\n",
    "    transaction_df = transaction_df.replace(\" \", np.NaN)\n",
    "    transaction_df = transaction_df.fillna(transaction_df.mean())\n",
    "    transaction_df[\"TransactionMonth\"] = transaction_df[\"transaction_date\"].apply(\n",
    "        get_month\n",
    "    )\n",
    "    transaction_df[\"TransactionYear\"] = transaction_df[\"transaction_date\"].dt.year\n",
    "    transaction_df[\"TransactionMonth\"] = transaction_df[\"transaction_date\"].dt.month\n",
    "    for col in transaction_df.columns:\n",
    "        if transaction_df[col].dtype == \"object\":\n",
    "            transaction_df[col] = transaction_df[col].fillna(\n",
    "                transaction_df[col].value_counts().index[0]\n",
    "            )\n",
    "\n",
    "    # Create transaction_date column based on month and store in TransactionMonth\n",
    "    transaction_df[\"TransactionMonth\"] = transaction_df[\"transaction_date\"].apply(\n",
    "        get_month\n",
    "    )\n",
    "    # Grouping by customer_id and select the InvoiceMonth value\n",
    "    grouping = transaction_df.groupby(\"customer_id\")[\"TransactionMonth\"]\n",
    "    # Assigning a minimum InvoiceMonth value to the dataset\n",
    "    transaction_df[\"CohortMonth\"] = grouping.transform(\"min\")\n",
    "\n",
    "    return transaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0bb54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/2pq06n0j76q0dwfrs31qvg240000gn/T/ipykernel_5903/3127388001.py:17: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  transaction_df = transaction_df.fillna(transaction_df.mean())\n",
      "/var/folders/p_/2pq06n0j76q0dwfrs31qvg240000gn/T/ipykernel_5903/3127388001.py:17: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  transaction_df = transaction_df.fillna(transaction_df.mean())\n"
     ]
    }
   ],
   "source": [
    "transaction_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17bebc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark",
   "language": "python",
   "name": "snowpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
